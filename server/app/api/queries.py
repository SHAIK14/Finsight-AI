from fastapi import APIRouter,Depends,HTTPException,status
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List,Optional
import json

from app.core.config import get_settings
from app.core.auth import get_current_user
from app.services.supabase_client import supabase
from app.services.document_processor import process_document
from app.services.vector_search import embed_question,search_similar_chunks
from app.services.llm_service import generate_answer
from app.services.reranker import rerank_chunks
from app.services.query_preprocessor import preprocess_query
from app.services.redis_cache import cache_service

router = APIRouter(
    prefix = "/api/queries",
    tags = ["queries"]
)

settings = get_settings() 

class QueryRequest(BaseModel):
    question:str
    document_ids:Optional[List[str]] = None

@router.post("/ask")
async def ask_question(
    request: QueryRequest,
    current_user: dict = Depends(get_current_user)
):
    user_id = current_user["clerk_id"]
    question = request.question
    doc_ids = request.document_ids or []

    async def event_generator():
        try:
            if current_user["role"] == "free":
                if current_user["queries_this_month"] >= settings.free_query_limit:
                    yield f"data: {json.dumps({'type': 'error', 'content': 'Query limit reached'})}\n\n"
                    return

            doc_query = supabase.table("documents").select("*").eq("clerk_id", current_user["clerk_id"])
            
            if request.document_ids:
                doc_query = doc_query.in_("id", request.document_ids)

            docs_response = doc_query.execute()

            if not docs_response.data:
                yield f"data: {json.dumps({'type': 'error', 'content': 'No documents found'})}\n\n"
                return

            actual_doc_ids = [doc["id"] for doc in docs_response.data]

            print(f"üîç Checking cache for question: {question}")
            cached_response = cache_service.get_query_response(user_id, question, actual_doc_ids)
            if cached_response:
                print(f"‚úÖ CACHE HIT - Returning cached response")
                answer_text = cached_response.get('answer', '')
                print(f"üìù Answer length: {len(answer_text)} characters")
                print(f"üìù First 100 chars: {answer_text[:100]}")

                # Stream in chunks of 5 characters for smooth appearance
                chunk_size = 5
                chunk_count = 0
                for i in range(0, len(answer_text), chunk_size):
                    chunk = answer_text[i:i+chunk_size]
                    chunk_count += 1
                    yield f"data: {json.dumps({'type': 'token', 'content': chunk})}\n\n"

                print(f"üì§ Sent {chunk_count} chunks")
                yield f"data: {json.dumps({'type': 'done', 'sources': cached_response.get('sources', [])})}\n\n"
                print(f"‚úÖ Done event sent")
                return

            print(f"‚ùå CACHE MISS - Running full pipeline")

            for doc in docs_response.data:
                if doc["status"] == "pending":
                    await process_document(doc["id"], supabase)

            normalized_query = preprocess_query(request.question)

            cached_chunks = cache_service.get_search_chunks(normalized_query, actual_doc_ids)

            if cached_chunks:
                reranked_chunks = cached_chunks
            else:
                question_embedding = embed_question(normalized_query)
                chunks = search_similar_chunks(
                    supabase=supabase,
                    question_embedding=question_embedding,
                    document_id=actual_doc_ids,
                    top_k=20
                )
                reranked_chunks = rerank_chunks(
                    query=request.question,
                    chunks=chunks,
                    top_n=5
                )
                cache_service.set_search_chunks(normalized_query, actual_doc_ids, reranked_chunks)

            full_answer = ""
            for event in generate_answer(
                question=request.question,
                chunks=reranked_chunks,
                stream=True
            ):
                if event.get('type') == 'token':
                    full_answer += event.get('content', '')
                yield f"data: {json.dumps(event)}\n\n"

            print(f"üíæ Storing in cache: {full_answer[:50]}...")
            cache_service.set_query_response(
                user_id=user_id,
                question=question,
                document_ids=actual_doc_ids,
                response={"answer": full_answer, "sources": reranked_chunks}
            )
            print(f"‚úÖ Cached successfully")
            
            # Update query count after streaming completes
            try:
                supabase.table("users").update({
                    "queries_this_month": current_user["queries_this_month"] + 1
                }).eq("id", current_user["id"]).execute()
            except Exception:
                pass
                
        except Exception as e:
            error_event = {
                "type": "error",
                "content": str(e)
            }
            yield f"data: {json.dumps(error_event)}\n\n"
    
    # Return StreamingResponse with the generator
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )
@router.delete("/cache/flush")
async def flush_cache(current_user: dict = Depends(get_current_user)):
    """Flush all cached queries for current user"""
    deleted_count = cache_service.invalidate_query_cache(current_user["clerk_id"])
    return {
        "success": True,
        "message": f"Flushed {deleted_count} cached queries"
    }

@router.get("/history")
async def get_history(
    limit: int = 50,
    current_user:dict = Depends(get_current_user)):
        return {
        "queries": [],
        "message": "Query history not yet implemented"
    }



    
    
        
        
        
        


        


 
